{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b44551e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Hypothesis: We can use the https://arxiv.org/pdf/2406.04313 method for increasing honesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198de680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T02:34:01.879987Z",
     "start_time": "2022-06-28T02:34:01.864103Z"
    }
   },
   "outputs": [],
   "source": [
    "# autoreload your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import adapter_overseer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d34518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a372ed7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T02:34:02.470436Z",
     "start_time": "2022-06-28T02:34:02.424826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/adapters_can_monitor_lies/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\"ignore\", \".*divide by zero.*\")\n",
    "\n",
    "## numeric, plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4)\n",
    "\n",
    "## utils\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import logging, os, re\n",
    "import collections, functools, itertools\n",
    "from loguru import logger\n",
    "\n",
    "from typing import List, Callable, Tuple, Dict, Optional\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "# torch\n",
    "# import pytorch_lightning as pl\n",
    "from einops import rearrange, repeat, reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from baukit.nethook import get_module\n",
    "from baukit import TraceDict\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64611cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractConfig(datasets=('amazon_polarity',), datasets_ood='imdb', model='failspy/Llama-3-8B-Instruct-abliterated', collection_layers=('base_model.model.model.layers.10', 'base_model.model.model.layers.20'), batch_size=2, prompt_format=None, num_shots=2, max_length=776, max_examples=1000, seed=42, max_epochs=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adapter_overseer.config import ExtractConfig\n",
    "\n",
    "cfg = ExtractConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a03c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd50635",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf63b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1ee3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/blog/mlabonne/orpo-llama-3\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "torch_dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64890012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T02:34:02.890216Z",
     "start_time": "2022-06-28T02:34:02.882249Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch_dtype, bnb_4bit_use_double_quant=True,)\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model, device_map=\"auto\", quantization_config=quantization_config,)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f98e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://old.reddit.com/r/LocalLLaMA/comments/1coizjy/tokenizer_config_of_llama3_changed_by_meta_in_hf/\n",
    "tokenizer.eos_token # it's good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "# \\peft_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import prepare_model_for_int8_training\n",
    "# # we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in float32 for stability. We also cast the output of the last layer in float32 for the same reasons.\n",
    "# model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"proj_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4da6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "# https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py\n",
    "config = LoraConfig(\n",
    "                        #r=32, lora_alpha=64, \n",
    "                    # target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\"\n",
    "                    )\n",
    "\n",
    "\n",
    "# LoRA config\n",
    "# peft_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "# )\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d102e3d",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps use load_preproc_datasets from sdb_probes_are_lie_detectors repo... /media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/prompts/prompt_loading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ae82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset of paired prompts, to try and get the model to lie\n",
    "from adapter_overseer.prompts.prompt_loading import load_preproc_datasets\n",
    "\n",
    "N = cfg.max_examples\n",
    "ds_tokens = load_preproc_datasets(\n",
    "    cfg.datasets,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e535a6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca812ab",
   "metadata": {},
   "source": [
    "## Train: transformers\n",
    "\n",
    "https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change the loss function!\n",
    "# we need to modify the forward pass, so that it returns a different loss function\n",
    "# but to calculate this we will need to residuals now, and as they werre\n",
    "# loss_bad = mse(repr_current, repr_target)\n",
    "\n",
    "# from transformers import SFTTrainer\n",
    "from trl.trainer import SFTTrainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from adapter_overseer.helpers.torch_helpers import clear_mem, switch\n",
    "from adapter_overseer.helpers.scores import select_choices\n",
    "\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    \"\"\"\n",
    "    Custom SFTTrainer that orthoganalizes the repr of bad examples, and retains good repr of examples\n",
    "\n",
    "    See: https://arxiv.org/pdf/2406.04313\n",
    "\n",
    "    args:\n",
    "        collection_layers: list of baukit layer names to collect\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, collection_layers: list, alpha=0.1, **kwargs):\n",
    "        super(CustomSFTTrainer, self).__init__(*args, **kwargs)\n",
    "        self.collection_layers = collection_layers\n",
    "        self.alpha = alpha\n",
    "        self.total_steps = self.args.max_steps\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "       # get label and prediction tokens\n",
    "\n",
    "        # collect the residuals of the model\n",
    "        # `list(model.named_modules())` for layer names\n",
    "        with TraceDict(model, self.collection_layers, detach=True) as ret_orig:\n",
    "            with model.disable_adapter():\n",
    "                orig_outputs = model(**inputs)\n",
    "        with TraceDict(model, self.collection_layers, retain_grad=True) as ret:\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "        # so now we have a mixed batch of good and bad outputs\n",
    "        # TODO get probs of each choice\n",
    "        # compare to labels to seperate into good and bad\n",
    "        # FIXME: but what if there are no lies in a batch...?\n",
    "        choice_ids = input['choice_ids'].detach().cpu().long()\n",
    "        label_instructed = input['label_true'] ^ input['instructed_to_lie']\n",
    "        label_true = input['label_true']\n",
    "\n",
    "        # does the underlying model get it right or wrong?\n",
    "        end_logits = orig_outputs[\"logits\"][:, -1].detach().cpu().float()\n",
    "        probs = torch.softmax(end_logits, -1)\n",
    "        choice_probs = select_choices(probs, choice_ids).sum(2)\n",
    "        binary_ans = choice_probs[:, 1] / (choice_probs.sum(1) + 1e-12)\n",
    "        correct_truth_telling = switch(binary_ans, label_true)\n",
    "        # correct_instruction_following = switch(binary_ans, label_instructed)\n",
    "\n",
    "        maks_desired = correct_truth_telling==1\n",
    "\n",
    "        # TODO concat these\n",
    "        for k in ret.keys():\n",
    "            activation = rearrange(ret[k].output, 'l b t h -> b l t h')[:, :, -1, :]\n",
    "            activation_orig = rearrange(ret_orig[k].output, 'l b t h -> b l t h')[:, :, -1, :].detach()\n",
    "\n",
    "\n",
    "        # get coeffecient\n",
    "        c = self.alpha * steps / 2 * self.total_steps\n",
    "        loss_retain = F.mse_loss(rep_orig, rep_adapt).mean()\n",
    "        loss_rr = F.relu(F.cosine_similarity(rep_orig, rep_adapt, dim=1)).mean()\n",
    "        loss = loss_rr * c + (1 - c) * loss_retain\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4003d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = CustomSFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds_tokens,\n",
    "    collection_layers=cfg.collection_layers,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        max_steps=20,\n",
    "        learning_rate=3e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c580dd",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "FIXME: lightning doesn't seem to play with with bnb\n",
    "\n",
    "\n",
    "note 16-true fails, 4bit helps?\n",
    "\n",
    "\n",
    "lightning options:\n",
    "- accelerator: gpu, or accelerate (seems to conflict with bnb)\n",
    "- precision, bf16-true (fails),bf16-mixed (uses more ram, likely because it undoes bnb)\n",
    "- using the [BitsandbytesPrecision](https://github.com/Lightning-AI/pytorch-lightning/blob/06ea3a05716a6d1f4a96cfb25021accdd18d8146/docs/source-fabric/fundamentals/precision.rst#quantization-via-bitsandbytes) plugin? But how does this work with lora?\n",
    "\n",
    "https://github.com/Lightning-AI/lit-llama/blob/main/finetune/adapter_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af653ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_train = DataLoader(\n",
    "            ds_tokens.with_format(\"torch\"), batch_size=cfg.batch_size, drop_last=False, shuffle=True, \n",
    "            # num_workers=cfg.num_workers,\n",
    ")\n",
    "dl_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e687f27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapter_overseer.train.pl_lora_ft import AtapterFinetuner\n",
    "import lightning as pl\n",
    "\n",
    "pl_model = AtapterFinetuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    total_steps=len(dl_train) * cfg.max_epochs,\n",
    "    collection_layers=cfg.collection_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    precision='16-mixed',\n",
    "\n",
    "    # gradient_clip_val=20,\n",
    "    devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    accumulate_grad_batches=8,\n",
    "    max_epochs=cfg.max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    # plugins=precision,\n",
    "    # enable_model_summary=False,\n",
    ")\n",
    "trainer.fit(model=pl_model, train_dataloaders=dl_train, \n",
    "            # val_dataloaders=dl_val\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b97c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
